# Milestone 2 – Infosys CodeGenie: AI Code Generation Benchmark

## Overview

This project implements **AI-driven code generation benchmarking** using multiple large language models (LLMs) to generate Python code for given prompts. The goal is to:

* Generate Python code for diverse programming tasks.
* Compute **metrics**: generation time, cyclomatic complexity, maintainability, and lines of code (LOC).
* Compare multiple AI models to understand trade-offs in **speed** vs **code quality**.
* Provide **interactive UIs** to run benchmarks and visualize results.

---

## Models Tested

| Model Name          | Hugging Face Repository                    |
| ------------------- | ------------------------------------------ |
| DeepSeek-Coder-1.3B | `deepseek-ai/deepseek-coder-1.3b-instruct` |
| Phi-2-2.7B          | `microsoft/phi-2`                          |
| Gemma-2B-IT         | `google/gemma-2b-it`                       |
| Stable-Code-3B      | `stabilityai/stable-code-3b`               |
| Replit-Code-3B      | `replit/replit-code-v1-3b`                 |

> Each model is **loaded dynamically**, used to generate code, and **unloaded immediately after execution** to avoid exceeding GPU memory limits.

---

## Features

1. **Two Interactive UIs:**

   * **Run Benchmark for All Models:** Execute all models sequentially on a given prompt.
   * **Run Benchmark for Selected Models:** Choose specific models via checkboxes.

2. **Metrics Computed for Generated Code:**

   * **Generation Time (s):** Time taken by the model to generate code.
   * **Cyclomatic Complexity:** Measures branching complexity; lower is generally easier to maintain.
   * **Maintainability Index:** Higher values indicate easier-to-maintain code.
   * **Lines of Code (LOC):** Total number of lines in generated code.

3. **Visualization:**

   * Interactive **bar plots** comparing generation time, complexity, and maintainability across models.
   * Helps identify the **best performing model** for a given task.

---

## Sample Prompts

The models were tested across different programming domains:

| Domain             | Example Prompt                                                               |
| ------------------ | ---------------------------------------------------------------------------- |
| Basic Python       | `Write a Python program to find the factorial of a number using recursion.`  |
| Data Structures    | `Implement a stack class in Python with push, pop, and peek operations.`     |
| Algorithms         | `Write a Python program to implement binary search on a sorted list.`        |
| File Handling      | `Write a Python program to read a text file and count the number of words.`  |
| Object-Oriented    | `Create a Python class named BankAccount with deposit and withdraw methods.` |
| Machine Learning   | `Write Python code using scikit-learn to train a linear regression model.`   |
| Web Development    | `Write a Flask app with one route '/hello' returning 'Hello, World!'.`       |
| Database           | `Write Python code to connect to an SQLite database and insert sample data.` |
| Data Visualization | `Write Python code using matplotlib to plot a line chart of monthly sales.`  |
| Automation         | `Write Python code to rename all files in a folder sequentially.`            |

---

## Results Table Example

| Model               | Prompt    | Gen Time (s) | Complexity | Maintainability | LOC |
| ------------------- | --------- | ------------ | ---------- | --------------- | --- |
| DeepSeek-Coder-1.3B | Factorial | 17.31        | 2.0        | 92.11           | 9   |
| Phi-2-2.7B          | Factorial | 3.52         | 3.0        | 69.28           | 8   |
| Gemma-2B-IT         | Factorial | 10.56        | 2.0        | 88.20           | 26  |
| Stable-Code-3B      | Factorial | 18.23        | NaN        | NaN             | NaN |

> **Note:** Some models (like Stable-Code-3B) may output code in formats that prevent metrics computation (hence NaN values). However, the generated code is still included in the table for qualitative comparison.

---

## Observations

* **Generation Time:** Phi-2-2.7B is consistently the fastest across most prompts.
* **Code Complexity:** DeepSeek-Coder-1.3B and Gemma-2B-IT produce simpler code (lower cyclomatic complexity).
* **Maintainability:** DeepSeek-Coder-1.3B often scores higher, indicating more readable and maintainable code.
* **LOC:** Depends on the task; Gemma-2-IT sometimes produces longer code due to extra documentation or comments.

> ✅ **Interpretation:** For quick solutions, Phi-2-2.7B is preferred. For maintainable and readable code, DeepSeek-Coder-1.3B is better. Gemma-2-IT is good for detailed, fully commented solutions.

---

## Conclusion

This benchmark:

* Provides **quantitative and qualitative comparison** between top AI code generation models.
* Highlights trade-offs: **speed vs maintainability**.
* Can be extended for **more prompts**, **additional models**, or **different programming languages**.

> ✅ DeepSeek-Coder-1.3B → best overall for maintainable code
> ✅ Phi-2-2.7B → best for speed
> ✅ Gemma-2B-IT → detailed, commented solutions
> ✅ Stable-Code-3B → sometimes inconsistent format, metrics may be NaN
